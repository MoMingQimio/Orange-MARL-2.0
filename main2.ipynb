{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mixed import MixAgent\n",
    "from buffer import MultiAgentReplayBuffer\n",
    "from OrangeEnv import MultiAgentEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(scenario, n_agents,AI_rates,N_GAMES,MAX_STEPS,evaluate,init_prob,spatial_distirbution_list):\n",
    "\n",
    "    env = MultiAgentEnv(n_agents, scenario)\n",
    "    actor_dims = []\n",
    "\n",
    "    #waiting for adjustment  \n",
    "    for i in range(n_agents):\n",
    "        actor_dims.append(env.observation_space[i] * 2)\n",
    "    critic_dims = n_agents * 2\n",
    "\n",
    "    n_actions= 2\n",
    "\n",
    "    for AI_rate in AI_rates :\n",
    "        #这两个依然有超参数，不过不重要\n",
    "        maddpg_agents = MixAgent(actor_dims, critic_dims, n_agents, n_actions,\n",
    "                                fc1=64, fc2=64, AI_rate=AI_rate,\n",
    "                                alpha=0.01, beta=0.01, scenario=scenario,\n",
    "                                chkpt_dir='tmp/mixed/')\n",
    "        memory = MultiAgentReplayBuffer(10000 , critic_dims, actor_dims,\n",
    "                                n_actions, n_agents, batch_size=1000 )\n",
    "        if not evaluate:\n",
    "            if os.path.exists('tmp/mixed/' +scenario+ \"/\" + \"agent_\" + str(n_agents -1) + \"_target_critic\" ):\n",
    "                if os.path.exists('tmp/mixed/' +scenario+ \"/\" + \"agent_\" + str(n_agents) + \"_target_critic\" ):\n",
    "                    for filename in os.listdir('tmp/mixed/' +scenario+ \"/\"):\n",
    "                        os.remove('tmp/mixed/' +scenario+ \"/\" + filename)\n",
    "                else:\n",
    "                    maddpg_agents.load_checkpoint()\n",
    "\n",
    "        #variable for counting, which is not need to adjust.\n",
    "        total_steps = 0\n",
    "        score_history = []\n",
    "        best_score = 0\n",
    "        PRINT_INTERVAL = 100\n",
    "\n",
    "        if evaluate:\n",
    "            maddpg_agents.load_checkpoint()\n",
    "\n",
    "        for i in range(N_GAMES):\n",
    "            state, obs = env.reset(init_prob)\n",
    "            score = 0\n",
    "            done = [False]*n_agents\n",
    "            episode_step = 0\n",
    "            cooperate_counts = [init_prob]\n",
    "\n",
    "\n",
    "            actions = np.zeros([n_agents,2])\n",
    "            split = int(init_prob * n_agents)\n",
    "            actions[:split,0] = 1.0\n",
    "            actions[split:,1 ] = 1.0\n",
    "            _, _, reward, _, _ = env.step(np.array(actions)[:,0])\n",
    "            while not any(done):\n",
    "                # if evaluate:\n",
    "                #     env.render()\n",
    "                    #time.sleep(0.1) # to slow down the action for the video\n",
    "                if episode_step in spatial_distirbution_list:\n",
    "                    spatial_distirbution = pd.DataFrame(np.array(actions)[:, 0].reshape(int(np.sqrt(n_agents)),-1))\n",
    "                    spatial_distirbution.to_csv('results/spatial_distribution/' + scenario + '/'\n",
    "                                                + scenario\n",
    "                                                + \", evaluate = \" + str(evaluate)\n",
    "                                                + \", Agents_num = \"+ str(n_agents)\n",
    "                                                + (', AI-rate = %.2f' % AI_rate)\n",
    "                                                + (\", eposide= %d\" % episode_step)\n",
    "                                                + \" .csv\"  , index= False, header= None)\n",
    "                cooperate_count = np.count_nonzero(np.array(actions)[:,0] == 0)\n",
    "                actions = maddpg_agents.choose_action(obs,reward, actions)\n",
    "                state_, obs_, reward, done, info = env.step(np.array(actions)[:,0])\n",
    "                \n",
    "                # state = obs_list_to_state_vector(obs)\n",
    "                # state_ = obs_list_to_state_vector(obs_)\n",
    "                cooperate_counts.append(cooperate_count / n_agents)\n",
    "\n",
    "                if episode_step >= MAX_STEPS - 2:\n",
    "                    done = [True]*n_agents\n",
    "\n",
    "                memory.store_transition(obs, state, actions, reward, obs_, state_, done)\n",
    "\n",
    "                if total_steps % 100 == 0 and not evaluate:\n",
    "                    maddpg_agents.learn(memory)\n",
    "\n",
    "                obs = obs_\n",
    "                state = state_\n",
    "\n",
    "                score = sum(reward)\n",
    "                total_steps += 1\n",
    "                episode_step += 1\n",
    "\n",
    "            score_history.append(score)\n",
    "            avg_score = np.mean(score_history[-100:])\n",
    "            if not evaluate:\n",
    "                if avg_score > best_score:\n",
    "                    maddpg_agents.save_checkpoint()\n",
    "                    best_score = avg_score\n",
    "            if i % PRINT_INTERVAL == 0 and i > 0:\n",
    "                print('episode', i, 'average score {:.1f}'.format(avg_score))\n",
    "\n",
    "\n",
    "        results = pd.Series(cooperate_counts)\n",
    "        if os.path.exists('results/cooperate_counts/' + scenario + '/'+\"evaluate = \"+ str(evaluate) +\", Agents_num = \"+ str(n_agents) + (', AI-rate = %.2f' % AI_rate) + \" .csv\"  ):\n",
    "            results_last = pd.read_csv('results/cooperate_counts/' + scenario + '/'+\"evaluate = \"+ str(evaluate) +\", Agents_num = \"+ str(n_agents) + (', AI-rate = %.2f' % AI_rate) + \" .csv\"  ,header= None)\n",
    "            # results_last = pd.Series(results_last)\n",
    "            results = pd.concat([results_last,results],ignore_index=True)\n",
    "        results.to_csv('results/cooperate_counts/' + scenario + '/'+\"evaluate = \"+ str(evaluate) +\", Agents_num = \"+ str(n_agents) + (', AI-rate = %.2f' % AI_rate) + \" .csv\"  , index= False,header=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\PYTHON PROGRAM\\Orange-MARL-v2.01\\networks.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = T.tensor(state,dtype= T.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "The program is finished!\n"
     ]
    }
   ],
   "source": [
    "scenario = \"lattice\" #博弈所处的环境，一共分为lattice,ba,random,ws四种。\n",
    "n_agents = 400 #智能体的总数。\n",
    "\n",
    "AI_rates = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1] #智能体为强化学习者的比例。\n",
    "\n",
    "N_GAMES = 1 #一局演化重复的次数。\n",
    "MAX_STEPS = 3000 #一次演化最多会有多少步\n",
    "\n",
    "evaluate = True #控制训练的开关。\n",
    "\n",
    "init_prob = 0.5\n",
    "\n",
    "spatial_distirbution_list = np.arange(10) * MAX_STEPS / 10#调整输出结果\n",
    "\n",
    "main(scenario, n_agents,AI_rates,N_GAMES,MAX_STEPS,evaluate,init_prob,spatial_distirbution_list)\n",
    "print(\"The program is finished!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e83c2f7c0291c0c08c6b0a6ae057777a7d9bd38cc83faae02e68fbd1c5388ff"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('pytorh-gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
